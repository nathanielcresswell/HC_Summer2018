{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is purely for making Hadley Cell indices (HC Indices). Because of our focus on the Southern hemisphere it only takes DJF into account. \n",
    "\n",
    "Procedure:\n",
    "    1. Read netcdf containing monthly vwnd from 1948 to 2017\n",
    "    2. Sort DJF \n",
    "    3. Identify cells using DB clustering \n",
    "    4. take spatial means over the identified cells for each timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#############   Import all necessary packages    ################\n",
    "#################################################################\n",
    "\n",
    "import importlib\n",
    "import plotly\n",
    "import netCDF4 as nc\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py \n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import * \n",
    "\n",
    "import numpy as np \n",
    "from scipy.io import netcdf \n",
    "from scipy.stats import pearsonr\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "plotly.tools.set_credentials_file(username='ncresswell', api_key='XVFWb00wZKWyDJTrB2Dl')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "################   define reference variables    ##################\n",
    "###################################################################\n",
    "\n",
    "#reference points for defining various cells \n",
    "s_atlantic_ref = [-22.5,14.4]\n",
    "s_pacific_ref  = [-17.5,-75.5]\n",
    "s_indian_ref   = [-26.14, 113.15]\n",
    "n_atlantic_ref = [21.5, -16]\n",
    "n_pacific_ref  = [38, -123]\n",
    "\n",
    "#specify useful variables before running \n",
    "filename = 'C:\\\\Users\\\\Nathaniel\\\\Desktop\\\\Summer2018\\\\HC_Summer2018\\\\Data\\\\vwnd.mon.mean_1948-2017.nc'\n",
    "alt = 1000 \n",
    "hadley_threshold = 4\n",
    "\n",
    "#define clustering data\n",
    "db_epsilon     = 4.7\n",
    "db_min_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "### create useful functions and enter relevent parameters for indexing  ###\n",
    "###########################################################################\n",
    "\n",
    "#see_file_data will receive a file with its path as a string and print information about the file...thie file must be net cdf \n",
    "#              will not return anything \n",
    "def see_file_data(path):\n",
    "    temp = nc.Dataset(filename,'r')\n",
    "    print('FILENAME: ', filename, '\\n','\\n')\n",
    "    print('FILE HEADER: ', '\\n', temp,'\\n','\\n')\n",
    "    print('FILE VARIABLES: ','\\n',temp.variables)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#fix_lon will be given a data array and a longitude array \n",
    "#        will return arrays of longitude and data that have been reformatted\n",
    "def fix_lon(array_to_fix, lon_of_array):\n",
    "    \n",
    "    #make longitude from -180 to 180 degrees east \n",
    "    tmp_lon = lon_of_array\n",
    "    for n in range (tmp_lon.size):\n",
    "        if tmp_lon[n]>180:\n",
    "            tmp_lon[n] = lon_of_array[n]-360\n",
    "    \n",
    "    #reorient data to be centered at 0 degrees east \n",
    "    i_west    = np.where(tmp_lon<0)\n",
    "    i_east    = np.where(tmp_lon>0)\n",
    "    west      = tmp_lon[i_west]\n",
    "    east      = tmp_lon[i_east]\n",
    "    fixed_lon = np.array(np.hstack((west,east)))\n",
    "    \n",
    "    #make similar adjustments so that vwnd matches new longitude \n",
    "    vwnd_west   = np.squeeze(array_to_fix[:,:,i_west])\n",
    "    vwnd_east   = np.squeeze(array_to_fix[:,:,i_east])\n",
    "    fixed_array = np.concatenate((vwnd_west,vwnd_east), axis=2)\n",
    "        \n",
    "    return fixed_array,fixed_lon\n",
    "\n",
    "def find_DJF_1D(array_to_parse):\n",
    "            \n",
    "    DJF  = np.array([])\n",
    "    JJA  = np.array([])\n",
    "    \n",
    "    for n in range (0,array_to_parse.shape[0]):\n",
    "        \n",
    "        temp = None\n",
    "        \n",
    "        if (np.mod(n,12)==1 or np.mod(n,12)==11 or np.mod(n,12)==0):\n",
    "                      \n",
    "            if (DJF.size==0):\n",
    "                DJF  = array_to_parse[n]\n",
    "                DJF  = DJF.reshape(1)\n",
    "            else:\n",
    "                temp = array_to_parse[n].reshape(1)\n",
    "                DJF = np.concatenate((DJF,temp),0)\n",
    "   \n",
    "    return DJF\n",
    "\n",
    "#find_DJF will take an array of data and an array of \n",
    "#             will return 3D arrays of DJF and JJA arrays ----> Assumes Jan start \n",
    "def find_DJF(array_to_parse):\n",
    "                \n",
    "    DJF  = np.array([])\n",
    "    JJA  = np.array([])\n",
    "    \n",
    "    for n in range (0,array_to_parse.shape[0]):\n",
    "        \n",
    "        temp = None\n",
    "        \n",
    "        if (np.mod(n,12)==1 or np.mod(n,12)==11 or np.mod(n,12)==0):\n",
    "                      \n",
    "            if (DJF.size==0):\n",
    "                DJF  = array_to_parse[n,:,:]\n",
    "                DJF  = DJF.reshape(1,array_to_parse.shape[1],array_to_parse.shape[2])\n",
    "            else:\n",
    "                temp = array_to_parse[n,:,:].reshape(1,array_to_parse.shape[1],array_to_parse.shape[2])\n",
    "                DJF  = np.concatenate((DJF,temp),0)\n",
    "   \n",
    "        temp = None\n",
    "\n",
    "    return DJF #,JJA\n",
    "\n",
    "#read_nc_file will be given the path to a netcdf file\n",
    "#             will return an array of time, level, lat, lon, data\n",
    "#                  >lon, lat and data will be adjusted so lon is from -180 to 180 degrees east, lat is 0 to 360 degrees north\n",
    "def read_nc_file( filename ):\n",
    "    \n",
    "    print('Reading netCDF file...')\n",
    "    \n",
    "    #extract lon, lat, level and data stored in the netCDF file specified \n",
    "    with nc.Dataset(filename,'r') as f:\n",
    "        lon   = f.variables['lon'][::]\n",
    "        lat   = f.variables['lat'][::-1]\n",
    "        time  = f.variables['time'][::]\n",
    "        level = f.variables['level'][::]\n",
    "        temp0 = f.variables['vwnd'][:,:,::-1,:]\n",
    "\n",
    "    #find index of appropriate pressure \n",
    "    index = np.where(level == alt)\n",
    "    \n",
    "    temp1 = temp0[:,index,:,:]\n",
    "    vwnd  = temp1.squeeze()\n",
    "      \n",
    "    vwnd,lon = fix_lon(vwnd,lon)\n",
    "    \n",
    "    return time, level, lat, lon, vwnd\n",
    "\n",
    "#find_clusters_labels will receive matrix of mean wind data\n",
    "#                     will return points that have been clustered (using density-base clustering) and their respective labels \n",
    "def find_clusters_labels(data):\n",
    "    \n",
    "    print('finding clusters with epsilon: ', db_epsilon,' for ', db_min_samples, ' minimum samples with threshold of ', hadley_threshold,'...')\n",
    "    \n",
    "    mask_coordinates = np.empty([2,1])\n",
    "    \n",
    "    first_element = True\n",
    "    \n",
    "    for n in range(0,data.shape[0]):\n",
    "        for m in range(0, data.shape[1]):\n",
    "            if np.absolute(data[n,m])>hadley_threshold:\n",
    "                temp = np.array([[lat[n]],[lon[m]]])\n",
    "                if first_element: \n",
    "                    mask_coordinates = temp \n",
    "                    first_element = False \n",
    "                else: \n",
    "                    mask_coordinates = np.append(mask_coordinates,temp,1)\n",
    "    \n",
    "    mask_coordinates = np.transpose(mask_coordinates)\n",
    "    db = DBSCAN(eps=db_epsilon, min_samples=db_min_samples).fit(mask_coordinates)\n",
    "    \n",
    "    return mask_coordinates, db.labels_\n",
    "\n",
    "#get_label will receive a reference point and an array of cluster points and their labels\n",
    "#          will return an integer representing which cluster is associated with the given reference point \n",
    "def get_label(ref_point, cluster_points, cluster_point_labels):\n",
    "    \n",
    "    print('finding cluster label of cell in question...')\n",
    "    distances = np.empty([cluster_points.shape[0],1])\n",
    "    \n",
    "    for n in range(0, cluster_points.shape[0]):\n",
    "        temp         = np.linalg.norm(cluster_points[n,:]-ref_point)\n",
    "        distances[n] = temp\n",
    "    \n",
    "    dist_min  = np.amin(distances)\n",
    "    min_index = np.where(distances == dist_min)\n",
    "    \n",
    "    return cluster_point_labels[min_index[0]]\n",
    "\n",
    "#center_zero takes an array and returns on with the mean removed \n",
    "def center_zero(array):\n",
    "    \n",
    "    mean = np.mean(array)\n",
    "    return np.subtract(array,mean)\n",
    "\n",
    "#get_mask_trace will return an array like data that is populated with 1's when a point is in the \n",
    "#      same cluster as the ref point and 0's at all other places\n",
    "def get_mask_trace(DJF_mean, lat ,lon , ref_point):\n",
    "        \n",
    "    if ref_point[0]>0:\n",
    "        print('Get mask trace only equipped to find HC in the summer hemisphere')\n",
    "    elif ref_point[0]<0:\n",
    "        data = DJF_mean\n",
    "    else: \n",
    "        print('      something went wrong....')\n",
    "        return\n",
    "    \n",
    "    #find cluster points and their labels \n",
    "    cluster_coordinates, cluster_labels = find_clusters_labels(data)\n",
    "\n",
    "    label=get_label(ref_point, cluster_coordinates, cluster_labels)\n",
    "    mask = np.empty_like(data)\n",
    "    \n",
    "    in_cluster = False \n",
    "    index = None\n",
    "    \n",
    "    for n in range(0,data.shape[0]):\n",
    "        for m in range(0,data.shape[1]):\n",
    "            \n",
    "            for i in range(0,cluster_coordinates.shape[0]):\n",
    "                \n",
    "                temp_cluster = cluster_coordinates[i,:]\n",
    "                temp_point   = np.array([lat[n],lon[m]])\n",
    "                \n",
    "                if np.array_equal(temp_cluster, temp_point):\n",
    "                    in_cluster = True\n",
    "                    index = i\n",
    "            \n",
    "            if in_cluster:\n",
    "                if cluster_labels[index] == label:\n",
    "                    mask[n,m] = 1\n",
    "                else:\n",
    "                    mask[n,m] = 0\n",
    "            else: \n",
    "                mask[n,m] = 0\n",
    "            \n",
    "            in_cluster = False\n",
    "            index = None\n",
    "                        \n",
    "    return mask\n",
    "\n",
    "#calculate_index will be given data array of JJA or DJF as appropriate and a mask of desired cell\n",
    "#                will return a time series of indices for the masked cell \n",
    "def calculate_index(data, mask):\n",
    "                 \n",
    "    #initialize array for later...\n",
    "    index       = np.empty([1,data.shape[0]])\n",
    "    \n",
    "    for m in range(0, data.shape[0]):\n",
    "        temp                      = data[m,:,:]\n",
    "        temp                      = np.multiply(data[m,:,:],mask)\n",
    "        temp[np.where(temp == 0)] = np.nan #turns 0's into nan's to make use of numpy's nanmean function\n",
    "        \n",
    "        temp1 = np.nanmean(temp,1)\n",
    "#         print('***shape of index: ',index.shape)\n",
    "#         print('***size of index: ',index.size)\n",
    "#         print('***shape of nan mean temp1 along axis 0: ',np.nanmean(temp1,0).shape)\n",
    "#         print('***size of nan mean temp1 along axis 1 ',np.nanmean(temp1,0).size)\n",
    "        index[0,m] = np.nanmean(temp1,0)\n",
    "    return np.transpose(center_zero(index))\n",
    "\n",
    "#combine_DJF will take an array during DJF and combine them to represent a single season \n",
    "#            will return an array with the same spatial dimensions and 1/3 the time dimension \n",
    "def combine_DJF(data):\n",
    "    \n",
    "    #initialize new array\n",
    "    seasonal_data = np.array([np.nan])\n",
    "    \n",
    "    #assuming first entry is Jan \n",
    "    seasonal_data[0] = np.mean([data[0],data[1]])\n",
    "    #combine the rest of the months \n",
    "    for i in range(1,int(np.ceil(np.divide(data.shape[0],3)))):\n",
    "        \n",
    "        seasonal_data = np.append(seasonal_data,np.mean([data[(i*3)-1],data[i],data[i+1]]))\n",
    "    #return the combined array \n",
    "    return seasonal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading netCDF file...\n",
      "---->Finding indices for South Atlantic...\n",
      "finding clusters with epsilon:  4.7  for  4  minimum samples with threshold of  4 ...\n",
      "finding cluster label of cell in question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathaniel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:214: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---->Finding indices for South Pacific...\n",
      "finding clusters with epsilon:  4.7  for  4  minimum samples with threshold of  4 ...\n",
      "finding cluster label of cell in question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathaniel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:214: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---->Finding indices for Indian Ocean...\n",
      "finding clusters with epsilon:  4.7  for  4  minimum samples with threshold of  4 ...\n",
      "finding cluster label of cell in question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathaniel\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:214: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "####  THIS script is used to find indexes for difference hadley cells  ##\n",
    "#########################################################################\n",
    "\n",
    "#extract important variables from netCDF file at appropriate level\n",
    "time, level, lat, lon, vwnd = read_nc_file(filename)\n",
    "\n",
    "#find arrays for DJF and JJA \n",
    "DJF = find_DJF(vwnd)\n",
    "DJF_mean = np.mean(DJF,0)\n",
    "# JJA_mean = np.mean(JJA,0)\n",
    "\n",
    "# #find indices for south atlantic hadley cell \n",
    "print('---->Finding indices for South Atlantic...')\n",
    "s_atlantic_HC = calculate_index(DJF, get_mask_trace(DJF_mean, lat ,lon , s_atlantic_ref))\n",
    "\n",
    "# #find indices for south pacific hadley cell \n",
    "print('---->Finding indices for South Pacific...')\n",
    "s_pacific_HC = calculate_index(DJF, get_mask_trace(DJF_mean, lat ,lon , s_pacific_ref))\n",
    "\n",
    "# #find indices for indianocean hadley cell \n",
    "print('---->Finding indices for Indian Ocean...')\n",
    "indian_HC = calculate_index(DJF, get_mask_trace(DJF_mean, lat ,lon , s_indian_ref))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor_divide(12,5)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These indices first element is mean of Jan,feb 1948 and last represents mean of Dec 2016, Jan, Feb 2017. Only DJF is included for SH indices  \n",
    "combine_DJF(s_atlantic_HC).dump(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_atlantic_indices_YEARLY.npy')\n",
    "combine_DJF(s_pacific_HC).dump(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_pacific_indices_YEARLY.npy')\n",
    "combine_DJF(indian_HC).dump(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_indian_indices_YEARLY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = np.load(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_pacific_indices_YEARLY_extended.npy')\n",
    "sa = np.load(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_atlantic_indices_YEARLY_extended.npy')\n",
    "si = np.load(r'C:\\Users\\Nathaniel\\Desktop\\Summer2018\\HC_Summer2018\\Data\\SavedVariables\\s_indian_indices_YEARLY_extended.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622368.0\n",
      "1866384.0\n"
     ]
    }
   ],
   "source": [
    "print(time[0])\n",
    "print(time[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time_series will receive a tuple with an unspecified number of inputs and plot them together. it will also receive a title for the plot and dates for the x axis  \n",
    "#                 will have no return values. to call:  plot_timeseries([array, 'legend title'],'title',xticks)\n",
    "def plot_timeseries(to_plot,title,times):\n",
    "    \n",
    "    if to_plot[0][0].size != to_plot[1][0].size:\n",
    "        print('Time Series of unequal size...')\n",
    "    \n",
    "    #determine how many series are being plotted \n",
    "    num_series = len(to_plot)\n",
    "    \n",
    "    plot = np.array([num_series],dtype=object)\n",
    "    #create array to hold the names of various series \n",
    "    legend_names = []\n",
    "    \n",
    "    for i in range (0,num_series):\n",
    "        \n",
    "        line = plt.plot(to_plot[i][0])\n",
    "        legend_names.append(to_plot[i][1])\n",
    "        \n",
    "    plt.legend(legend_names)\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.xticks(np.arange(0,to_plot[0][0].size,5),times)\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20,5)\n",
    "    \n",
    "    if num_series == 2:\n",
    "        print('Correlaiton Coefficeint: ',np.corrcoef(to_plot[0][0],to_plot[1][0])[0,1])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
